---
layout: 'publication'
title: "Empowering Transformers for Evidence-Based Medicine"
collection: publications
type: 'preprint'
permalink: /publication/2023-transformers-ebm
excerpt: "This paper explores how transformer models like BERT and GPT can be leveraged for automatic question-answering (Q&A) in evidence-based medicine, particularly in responding to structured clinical questioning protocols like PICO."
date: 2023-12-25
venue: 'medRxiv preprint'
paperurl: 'https://doi.org/10.1101/2023.12.25.23300520'
authors: '<strong>Sabah Mohammed</strong>, <strong>Jinan Fiaidhi</strong>, <strong>Hashmath Shaik</strong>'
thumbnail: 'transformers-ebm.jpg'
videourl: '#'
abstract: 'Breaking the barrier to practicing evidence-based medicine relies on effective methods for rapidly identifying relevant evidence from biomedical literature. One of the main challenges for medical practitioners is the time required to browse, filter, and compile information from various medical sources. Deep learning techniques, particularly transformer-based models like BERT and GPT, can automate question-answering (Q&A) for clinical queries. However, existing transformer models are not specifically trained to address structured clinical questioning protocols like PICO (Patient/Problem, Intervention, Comparison, and Outcome). In this work, we present a two-stage bootstrapping process that enhances the extraction of relevant medical evidence. Our approach applies patch attention mechanisms to improve relevance filtering, achieving state-of-the-art performance in answering clinical questions using PubMed data.'
bibtex: '@article{mohammed2023transformers,
 <br> author = {Mohammed, Sabah and Fiaidhi, Jinan and Shaik, Hashmath},
 <br> title = {Empowering Transformers for Evidence-Based Medicine},
 <br> journal = {medRxiv preprint},
 <br> year = {2023},
 <br> publisher = {Cold Spring Harbor Laboratory Press},
 <br> pages = {2023.12.25.23300520}
<br> }'
---